{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3820b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58741b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "\t\"data/household_power_consumption.txt\",\n",
    "\tsep=';',\n",
    "\tlow_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdccf009",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d733213",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee2067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576b95df",
   "metadata": {},
   "source": [
    "We can see that data has inappropriate format (Dtype). All the column except Date and Time should be float. Computer will not work with strings, we try to avoid this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9bd757",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(df.columns)\n",
    "columns.remove(\"Date\")\n",
    "columns.remove(\"Time\")\n",
    "\n",
    "for column in columns:\n",
    "\tdf[column] = pd.to_numeric(df[column], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e948e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f786146b",
   "metadata": {},
   "source": [
    "To get away from string columns, we combine Date and Time columns and put them as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7847c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"full_time\"] = pd.to_datetime(df[\"Date\"] + ' ' + df[\"Time\"], dayfirst=True)\n",
    "\n",
    "df.set_index(\"full_time\", inplace=True)\n",
    "\n",
    "df.drop(columns=[\"Date\", \"Time\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfc1591",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621139f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2e1b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc81ecbb",
   "metadata": {},
   "source": [
    "We have to fill these NaN values. We can do it by replacing them with median or mean of the column. I think median is more appropriate, because electricity consumptions distribution is very skewed. Mean is very vulnurable to outliers or skewed distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407ed718",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(df.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1e42a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead9ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize=(15, 10), bins=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f369deab",
   "metadata": {},
   "source": [
    "from these graphs we can say that our data has a lot of outliers. Normal distribution has only Voltage feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0144bf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc516cb9",
   "metadata": {},
   "source": [
    "Standard train_test_split would be a bad option for us, because we work with sequential data. We must cut the data chronologically;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f2af6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_id = int(len(df) * 0.8)\n",
    "\n",
    "X = df.drop(columns=[\"Global_active_power\"])\n",
    "y = df[\"Global_active_power\"]\n",
    "\n",
    "X_train = X.iloc[:split_id, :]\n",
    "X_test = X.iloc[split_id:, :]\n",
    "\n",
    "y_train = y.iloc[:split_id]\n",
    "y_test = y.iloc[split_id:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ea35fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f0dac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ba67c4-5e62-4d98-8156-c4aa03a00237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X, y, seq_len=60):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        Xs.append(X[i:i+seq_len])\n",
    "        ys.append(y[i+seq_len])\n",
    "    return np.array(Xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a044b2c7-9359-40b2-9a0d-afd9821776fc",
   "metadata": {},
   "source": [
    "An LSTM cannot learn from single time steps. It needs a sequence of past observations to predict a future value so we convert time-series into sliding windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4e8aad-bb7a-4b83-91a1-0b1d8271dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f424a27b-6b32-4fa1-b9ea-6c711ec09255",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 60\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train.values, SEQ_LEN)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test.values, SEQ_LEN)\n",
    "\n",
    "X_train_t = torch.tensor(X_train_seq, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train_seq, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "X_test_t = torch.tensor(X_test_seq, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test_seq, dtype=torch.float32).unsqueeze(1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddea7873",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16adf9a6-02c2-4c9b-a7cf-8af82b000c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(\n",
    "    input_size=X_train_seq.shape[2],\n",
    "    hidden_size=64,\n",
    "    output_size=1,\n",
    "    num_layers=2\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8e3f09-5092-4b82-b166-10d33b47a9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db80895-3f7b-4a35-99b0-e3194ab40987",
   "metadata": {},
   "source": [
    "As a loss function we use Mean Squared Error (MSE) because it is regression task. Also it penalizes larger errors more strongly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b6808c-804f-4557-b3c1-8b973d06d4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    permutation = torch.randperm(X_train_t.size(0))\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i in range(0, X_train_t.size(0), BATCH_SIZE):\n",
    "        indices = permutation[i:i+BATCH_SIZE]\n",
    "\n",
    "        batch_x = X_train_t[indices]\n",
    "        batch_y = y_train_t[indices]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # ETA calculation\n",
    "        batches_done = i + BATCH_SIZE\n",
    "        batches_total = X_train_t.size(0)\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        progress = min(batches_done / batches_total, 1.0)\n",
    "        eta = elapsed * (1 - progress) / progress if progress > 0 else 0\n",
    "\n",
    "        print(\n",
    "            f\"\\rEpoch {epoch+1}/{EPOCHS} \"\n",
    "            f\"- Loss: {epoch_loss:.4f} \"\n",
    "            f\"- ETA: {eta:.1f}s\",\n",
    "            end=\"\"\n",
    "        )\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969ed5b1-4318-4d7b-a2ba-180e21a0f0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_t).numpy()\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test_seq, y_pred))\n",
    "mae = mean_absolute_error(y_test_seq, y_pred)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797843f1-6f6e-4247-b084-ad8661aac487",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(y_test_seq[:1000], label=\"True\")\n",
    "plt.plot(y_pred[:1000], label=\"Predicted\")\n",
    "plt.legend()\n",
    "plt.title(\"Global Active Power Prediction\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
